{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7kEGGMRKQBdl"
   },
   "source": [
    "# SMM with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RDGR-iaV3vf1"
   },
   "source": [
    "## Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlFGxFZ997P4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4zgNC-G3xwY"
   },
   "source": [
    "## Data Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE THE PATH TO YOUR COURSE DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hannah's partition scheme and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_blosum(filename):\n",
    "    \"\"\"\n",
    "    Read in BLOSUM values into matrix.\n",
    "    \"\"\"\n",
    "    aa = ['A', 'R', 'N' ,'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V', 'X']\n",
    "    df = pd.read_csv(filename, sep=' ', comment='#', index_col=0)\n",
    "    return df.loc[aa, aa]\n",
    "\n",
    "def load_peptide_target(filename, MAX_PEP_SEQ_LEN=9):\n",
    "    \"\"\"\n",
    "    Read amino acid sequence of peptides and\n",
    "    corresponding log transformed IC50 binding values from text file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename, sep=' ', usecols=[0,1], names=['peptide','target'])\n",
    "    return df[df.peptide.apply(len) <= MAX_PEP_SEQ_LEN]\n",
    "\n",
    "def load_pickle(f):\n",
    "    with open(f, 'rb') as source:\n",
    "        s = pickle.load(source)\n",
    "    return s\n",
    "\n",
    "def load_partitions(files):\n",
    "    o = []\n",
    "    for f in files:\n",
    "        data = load_pickle(f)\n",
    "        o += data\n",
    "    return o\n",
    "\n",
    "def assign_cv_partition(partition_files, n_folds=5, n_test=1):\n",
    "    \"\"\"Figure out all combinations of partition_files to assign as train and test data in CV\"\"\"\n",
    "\n",
    "    # how many combinations of partition_files in train part\n",
    "    n_train = n_folds - n_test\n",
    "\n",
    "    # find all combinations of the partition_files with n_train files in each\n",
    "    train_files = list(combinations(partition_files, n_train))\n",
    "\n",
    "    # convert each list element to tuple so (train_partitions, test_partition)\n",
    "    files = [\n",
    "        (x, list(set(partition_files) - set(x))) for x in train_files\n",
    "    ]\n",
    "\n",
    "    return files\n",
    "\n",
    "def data_partition(partition_files, data, blosum_file, batch_size=32, n_features=9):\n",
    "    partitions = load_partitions(partition_files)\n",
    "\n",
    "    selected_data = data.loc[data.peptide.isin(partitions), ].reset_index()\n",
    "\n",
    "    X, y = encode_peptides(selected_data, blosum_file=blosum_file, batch_size=batch_size, n_features=n_features)\n",
    "\n",
    "    # reshape X\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def encode_peptides(Xin, blosum_file, batch_size, n_features, MAX_PEP_SEQ_LEN=9):\n",
    "    \"\"\"\n",
    "    Encode AA seq of peptides using BLOSUM50.\n",
    "    Returns a tensor of encoded peptides of shape (batch_size, MAX_PEP_SEQ_LEN, n_features)\n",
    "    \"\"\"\n",
    "    blosum = load_blosum(blosum_file)\n",
    "    \n",
    "    batch_size = len(Xin)\n",
    "    n_features = len(blosum)\n",
    "    \n",
    "    Xout = np.zeros((batch_size, MAX_PEP_SEQ_LEN, n_features), dtype=np.int8) # should it be uint? is there a purpose to that?\n",
    "    \n",
    "    for peptide_index, row in Xin.iterrows():\n",
    "        for aa_index in range(len(row.peptide)):\n",
    "            Xout[peptide_index, aa_index] = blosum[ row.peptide[aa_index] ].values\n",
    "            \n",
    "    return Xout, Xin.target.values\n",
    "\n",
    "data = load_peptide_target('data/A0201/A0201.dat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uzw3jsGnzw0e"
   },
   "source": [
    "### Alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYcVU4VLzyVe"
   },
   "outputs": [],
   "source": [
    "alphabet_file = data_dir + \"Matrices/alphabet\"\n",
    "alphabet = np.loadtxt(alphabet_file, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww-p6gy81mqk"
   },
   "source": [
    "## Error Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hv2nx4Yq1lBf"
   },
   "outputs": [],
   "source": [
    "def cumulative_error(peptides, y, lamb, weights):\n",
    "\n",
    "    error = 0\n",
    "    \n",
    "    for i in range(0, len(peptides)):\n",
    "        \n",
    "        # get peptide\n",
    "        peptide = peptides[i]\n",
    "\n",
    "        # get target prediction value\n",
    "        y_target = y[i]\n",
    "        \n",
    "        # get prediction\n",
    "        y_pred = np.dot(peptide, weights)\n",
    "            \n",
    "        # calculate error\n",
    "        error += 1.0/2 * (y_pred - y_target)**2\n",
    "        \n",
    "    gerror = error + lamb*np.dot(weights, weights)\n",
    "    error /= len(peptides)\n",
    "        \n",
    "    return gerror, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict value for a peptide list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(peptides, weights):\n",
    "\n",
    "    pred = []\n",
    "    \n",
    "    for i in range(0, len(peptides)):\n",
    "        \n",
    "        # get peptide\n",
    "        peptide = peptides[i]\n",
    "        \n",
    "        # get prediction\n",
    "        y_pred = np.dot(peptide, weights)\n",
    "        \n",
    "        pred.append(y_pred)\n",
    "        \n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate MSE between two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mse(vec1, vec2):\n",
    "    \n",
    "    mse = 0\n",
    "    \n",
    "    for i in range(0, len(vec1)):\n",
    "        mse += (vec1[i] - vec2[i])**2\n",
    "        \n",
    "    mse /= len(vec1)\n",
    "    \n",
    "    return( mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfvPqSjL2g7u"
   },
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HMXHiHmE2gh9"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(y_pred, y_target, peptide, weights, lamb_N, epsilon):\n",
    "    \n",
    "    # do is dE/dO\n",
    "    #do = XX\n",
    "    do = y_pred - y_target\n",
    "    \n",
    "    for i in range(0, len(weights)):\n",
    "        \n",
    "        #de_dw_i = XX\n",
    "        de_dw_i = do * peptide[i] + 2 * lamb_N * weights[i]\n",
    "        #weights[i] -= XX\n",
    "        weights[i] -= epsilon * de_dw_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make storage for inner loop predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_storage(hyper_parameter_1, hyper_parameter_2):\n",
    "    store_predictions={}\n",
    "\n",
    "    for i in hyper_parameter_1:\n",
    "    \n",
    "        store_predictions[i]={}\n",
    "    \n",
    "        for j in hyper_parameter_2:\n",
    "        \n",
    "            store_predictions[i][j] = []\n",
    "    return(store_predictions)\n",
    "\n",
    "#storage_matrix = make_storage([\"a\",\"b\",\"c\"],[1,2,3])\n",
    "#print(storage_matrix)\n",
    "#storage_matrix[\"a\"][1].extend([1,1,1])\n",
    "#storage_matrix[\"a\"][1].extend([1,1,1])\n",
    "\n",
    "#for i in storage_matrix:\n",
    "    \n",
    "#    for j in storage_matrix[\"a\"]:\n",
    "#        print(i,j)\n",
    "#        print(storage_matrix[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHXm8HAm4S_u"
   },
   "source": [
    "## Main Loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1717
    },
    "colab_type": "code",
    "id": "EcHQYE2sja-y",
    "outputId": "3939a58c-88ac-4ae1-b680-edc4c2c913a1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best PCC  0.6739683036523925 Best Lamba  0.01 Best epsilon  5e-05\n",
      "Lambda:  0.01 Epsilon:  5e-05 PCC:  0.6753766990688159\n",
      "[ 1.88654586e-02 -7.99511344e-03  4.70978560e-02  2.14276814e-02\n",
      " -2.90107648e-03 -3.80472815e-02 -2.47586470e-02 -8.95980776e-03\n",
      " -1.38490784e-02  1.14259772e-02 -1.48197360e-02 -5.12297589e-03\n",
      "  2.78310899e-02 -6.28421634e-02  1.72622612e-02  5.46807381e-02\n",
      "  7.89391816e-03 -1.09477069e-02 -4.69931277e-02  6.02971617e-02\n",
      " -1.75271068e-02 -5.63609308e-02  1.00965170e-02  4.17310626e-02\n",
      "  4.44429108e-02 -6.55141684e-03 -5.19110470e-02  7.73798400e-03\n",
      " -5.54004969e-02 -2.39800151e-02  7.00703658e-02 -2.19863023e-02\n",
      " -5.44370321e-02 -2.92547388e-02  2.58525122e-03 -2.06768440e-02\n",
      "  3.30272408e-02  3.15253071e-02  1.00151394e-02  3.57248029e-02\n",
      " -4.71717687e-02  4.95530604e-02 -9.84148196e-04 -1.88510135e-02\n",
      "  6.48326433e-03  1.37369808e-02 -6.72010958e-03  2.77293470e-03\n",
      " -1.71433709e-03 -3.77337619e-02 -7.56517788e-04  5.56866725e-02\n",
      " -4.51622892e-02 -8.18497118e-03 -1.15015661e-02 -1.06337518e-02\n",
      " -1.48028685e-02 -1.60612763e-03 -8.96480893e-04 -2.50051648e-03\n",
      " -1.07469984e-02  4.51770725e-04  7.00650350e-02  8.84683892e-03\n",
      " -3.65015846e-03 -1.52914256e-02 -1.64439384e-02  1.65771622e-05\n",
      " -1.09914853e-02 -2.94612491e-02 -1.25959374e-02 -1.57663346e-02\n",
      "  1.01354440e-02 -2.01973721e-02  1.29194582e-02 -8.57282602e-03\n",
      "  4.76287527e-02 -2.05282224e-02 -6.52090202e-02 -3.29301896e-02\n",
      "  4.70823193e-03 -2.15716728e-02 -2.03099142e-02  1.55167456e-02\n",
      " -3.80875857e-04 -1.80582655e-02  2.23183438e-02 -3.18027276e-03\n",
      " -7.03996820e-03 -1.61731617e-02 -1.14142139e-02  1.71433119e-02\n",
      "  3.58238232e-02  1.44080611e-02 -4.33431585e-02 -1.01809626e-02\n",
      " -9.20048827e-03 -6.90710162e-02  2.10649218e-02  4.04710181e-02\n",
      "  1.63271015e-02 -9.17450251e-03  3.28560146e-02  2.49710280e-02\n",
      "  3.16317289e-03  6.71271889e-03 -2.65561490e-02 -6.17035381e-02\n",
      " -2.09649180e-02 -3.55858717e-03  3.96195547e-02 -6.92720082e-03\n",
      " -1.90017639e-02  6.19488519e-03  5.60201147e-02 -3.68213767e-02\n",
      "  2.77186589e-02 -1.93648581e-02 -2.83809708e-03 -5.72399421e-03\n",
      " -6.55281449e-03 -2.66093224e-02  6.89785223e-03 -3.04228151e-02\n",
      " -8.55190746e-03  6.96573188e-02 -7.70662604e-03  6.27482865e-03\n",
      "  4.01432219e-02 -3.24625312e-02 -1.14425176e-02 -4.85697174e-02\n",
      " -1.56671264e-02  5.27314185e-02  6.37373264e-02  3.74435436e-02\n",
      " -7.55083223e-02 -4.34353143e-02 -2.32772418e-02 -8.56090306e-02\n",
      "  2.63445564e-02  6.78336658e-02  3.80006712e-02 -1.26900066e-03\n",
      "  6.04106784e-02 -1.30210099e-02  5.31293956e-02 -3.40408152e-02\n",
      "  2.58747150e-03 -2.37718398e-02 -1.43569189e-02  4.23284312e-03\n",
      "  1.18732411e-02 -1.35491347e-02  9.68715496e-03  4.86899289e-03\n",
      "  1.65484866e-03 -6.62574324e-03  4.81549348e-04 -1.75561629e-02\n",
      "  8.10623508e-03  5.44079513e-03 -4.78953124e-03 -1.30584324e-02\n",
      "  8.01939931e-03  1.89368296e-02 -4.52095024e-03  7.60178183e-03\n",
      "  1.04061700e-02  2.65294399e-02  8.23945819e-02  4.50631504e-02\n",
      " -8.33543184e-03 -5.95252259e-02 -3.53851758e-02 -2.09451417e-02\n",
      " -6.77409780e-02 -6.09728557e-02  6.18008875e-02  1.55887031e-03\n",
      "  9.77185726e-03  1.40847943e-02 -1.31761155e-02  1.29074388e-02\n",
      "  1.94237936e-02  1.09895780e-02 -9.54562120e-03  3.16506042e-02\n",
      " -4.15144353e-02]\n",
      "Best PCC  0.7174267320725387 Best Lamba  0.1 Best epsilon  5e-05\n",
      "Lambda:  0.1 Epsilon:  5e-05 PCC:  0.719514711127585\n",
      "[-4.04435138e-02  9.50611929e-03 -9.14985938e-03  5.79545384e-02\n",
      "  4.60627685e-03  2.15388223e-02  2.83700910e-02 -2.87634268e-02\n",
      " -1.81261858e-02  9.43943023e-04  2.17623671e-02 -1.22334467e-02\n",
      "  1.23945529e-02 -2.97668000e-02  1.71525232e-02  7.52618219e-02\n",
      " -6.82680518e-03  7.85688970e-03 -1.15622524e-02  5.98840433e-02\n",
      " -5.90267581e-02  1.84890839e-02 -2.67531767e-02 -1.49834101e-02\n",
      "  2.85110302e-02  5.24427166e-05  7.70488024e-04 -3.63439255e-02\n",
      " -4.87489675e-02 -2.86482977e-02  3.29079493e-02  1.70685198e-02\n",
      "  2.33350123e-02 -2.00669997e-02 -6.77280073e-02  1.62042381e-02\n",
      "  6.86365056e-02 -3.21687622e-02  1.17357386e-02 -3.55037499e-02\n",
      "  1.97663098e-02  1.95354709e-02 -4.16541495e-03 -1.02945415e-02\n",
      "  1.83366489e-02  3.84507782e-02 -4.15872102e-03 -1.16262707e-02\n",
      " -2.15581653e-02 -4.75111653e-02 -5.56110123e-02 -1.60538678e-02\n",
      "  2.73037404e-02  6.22219074e-03 -5.99986148e-03  1.42635072e-02\n",
      " -2.82165591e-02 -2.86521520e-02 -2.76513674e-02  1.97692391e-03\n",
      " -3.44587395e-02  2.92585931e-02 -5.75185527e-03  4.63475474e-02\n",
      " -2.77635219e-02 -6.43546456e-02 -2.83682566e-02 -1.49181734e-03\n",
      "  4.06165837e-02 -3.95505538e-02  6.92294709e-03  9.44073766e-03\n",
      " -7.18150127e-03 -1.07290363e-02  5.27845109e-02 -9.43883838e-03\n",
      " -2.81234200e-02  1.44308736e-02 -1.43872223e-02 -6.70472231e-02\n",
      " -2.39455635e-03 -3.53260214e-02  2.85174323e-02 -1.69647231e-02\n",
      " -4.82649182e-02 -6.16391960e-03 -2.39876552e-02  1.87468181e-02\n",
      " -2.84164921e-03  2.76676912e-02  2.66915544e-02  2.55519089e-03\n",
      "  2.64182816e-02  1.35771945e-02 -2.17697000e-02 -1.84062605e-02\n",
      " -1.83942309e-02 -1.93749028e-02  1.24097482e-02  4.50338905e-02\n",
      "  3.69493163e-03  6.07259945e-03  5.59090604e-02  7.80467999e-03\n",
      " -1.44571512e-02 -1.54870099e-02 -1.68189009e-02 -1.41512793e-02\n",
      " -1.23126853e-02 -1.20320000e-02 -1.82899872e-03  1.99389254e-03\n",
      "  1.61480855e-02  4.34579559e-02  3.69989341e-02 -3.80102928e-02\n",
      " -2.19620063e-02 -3.06264622e-02 -8.48805719e-02  1.70612377e-02\n",
      "  5.49355343e-02  6.87736494e-04 -4.64369740e-03  4.31477409e-02\n",
      "  1.51519312e-02  2.19162209e-02 -2.09792469e-02 -6.24586996e-03\n",
      " -1.88417394e-02  2.30394256e-02  3.53720062e-04  4.48283972e-03\n",
      " -1.45529166e-02 -4.24107549e-02 -5.06693922e-02  4.52425334e-04\n",
      "  1.60231546e-02  2.15310460e-02 -1.89180497e-02  2.83208681e-02\n",
      " -2.11337849e-02 -3.02187686e-02 -4.08870006e-02  1.88150831e-02\n",
      " -3.41485524e-02  2.64588282e-02 -1.24513976e-03 -9.22653015e-03\n",
      " -1.61145184e-02 -9.10633775e-02 -3.35970864e-02  6.31297420e-03\n",
      "  7.58757459e-02  1.14268734e-02  1.60085504e-03  3.34060766e-02\n",
      "  5.75104048e-02 -5.13131838e-02  2.48195946e-02 -1.40887883e-02\n",
      "  9.22842243e-03  1.78536015e-02  1.45917157e-02 -2.85403217e-02\n",
      "  8.08600894e-03 -1.31826154e-02 -2.36928770e-02  7.32843700e-02\n",
      "  1.46800070e-02 -1.30404675e-02  5.67440633e-02  3.23646976e-02\n",
      " -3.23319219e-03 -2.71247763e-02 -3.73016530e-03 -4.73965837e-02\n",
      "  1.79575724e-03  4.76779368e-02 -4.29959938e-02  3.92332518e-03\n",
      "  4.87880558e-04 -7.25565406e-02  2.31516676e-02  7.13957727e-02\n",
      "  1.97532122e-02  8.97403495e-05  2.55973507e-02  5.70487731e-02\n",
      "  2.96808089e-02]\n",
      "Best PCC  0.757491568350865 Best Lamba  0.001 Best epsilon  5e-05\n",
      "Lambda:  0.001 Epsilon:  5e-05 PCC:  0.7563286309453136\n",
      "[ 0.03392213 -0.02245516  0.04741661  0.01737233 -0.00378096 -0.03498856\n",
      " -0.03862307 -0.03501581 -0.04045183  0.01941966 -0.01686078  0.02078553\n",
      "  0.03637044 -0.04032408  0.00142549  0.01297467 -0.00311296 -0.01043055\n",
      " -0.09032696  0.06791651 -0.00173181 -0.0382368   0.00965671  0.05383375\n",
      "  0.05406005 -0.00630325 -0.06534086 -0.02693935 -0.03875846 -0.04972408\n",
      "  0.00405962  0.03625112 -0.02807564 -0.02770637 -0.01064768 -0.01634117\n",
      "  0.03279893  0.01339659  0.01266101  0.01707133 -0.01703291 -0.00216325\n",
      " -0.04152015 -0.00809551 -0.00812583  0.05595843 -0.00704631  0.01973981\n",
      "  0.01975679 -0.04029631 -0.01921613 -0.00555631  0.02030667 -0.01928475\n",
      " -0.01678621 -0.02388928 -0.01136383  0.03169163 -0.01822277  0.00153115\n",
      "  0.02545288  0.03304083 -0.02477317  0.04057491 -0.01863657  0.00814893\n",
      " -0.01781895 -0.00385857 -0.03339843 -0.06654803  0.0147048  -0.02055585\n",
      " -0.04194484  0.01344763  0.03055666 -0.00298265 -0.01303947 -0.00630877\n",
      " -0.0503599  -0.04861271 -0.00670308 -0.02956803  0.03066282 -0.03625756\n",
      " -0.07503656  0.01283858  0.06140355  0.01470617 -0.01161767 -0.05282229\n",
      "  0.02601049  0.02793874  0.04461428  0.01672393 -0.0489232  -0.07300276\n",
      " -0.0181462  -0.04493197  0.0030484   0.04421646  0.057505   -0.0023371\n",
      "  0.11155276 -0.01841895 -0.00882423 -0.01431905 -0.00736352  0.0122482\n",
      " -0.0008554  -0.00119052 -0.02865341 -0.02909295 -0.03288659 -0.04857149\n",
      "  0.00906241  0.01126877  0.00889201 -0.01400674  0.03235037 -0.0342785\n",
      " -0.0536963  -0.01264948  0.00958298 -0.03414743 -0.0004269   0.02209962\n",
      "  0.05171111 -0.02829818  0.04108704 -0.017095   -0.01006426 -0.05306288\n",
      " -0.07207926  0.02449809  0.00100975 -0.01760463 -0.02212854  0.02708538\n",
      " -0.00840464 -0.0965083   0.02143543  0.02751113 -0.02425237 -0.00435069\n",
      " -0.02615279  0.06233274  0.00030664  0.01503302 -0.01154633 -0.02279016\n",
      " -0.02034617  0.00275343  0.00948157 -0.03433583  0.00090069 -0.00891515\n",
      " -0.00127398 -0.00201824  0.02765381 -0.00277283 -0.01424725  0.00844455\n",
      " -0.00836507 -0.0331923  -0.0002915  -0.02952325  0.0201316   0.0134925\n",
      " -0.02446142 -0.01659921  0.01801349  0.05349016  0.00544834  0.00755396\n",
      " -0.02660214 -0.04797394 -0.0017426   0.03546515 -0.03679782  0.04810241\n",
      " -0.01948009 -0.01889644  0.02333101  0.05693389 -0.01140029  0.01802963\n",
      "  0.05002157  0.04395653  0.0129086 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-7f89a4c591a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m                         \u001b[1;31m#print(y_target)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                         \u001b[1;31m# get initial prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeptide\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m                         \u001b[1;31m#print(y_pred)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                         \u001b[1;31m# gradient descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Lambdas\n",
    "lamb_list=[10,1,0.1,0.01,0.001]\n",
    "\n",
    "# learning rate, epsilon\n",
    "epsilon_list = [0.0005, 0.00005, 0.000005]\n",
    "\n",
    "# training epochs\n",
    "epochs = 100\n",
    "\n",
    "#Use Hannahs encoding loading\n",
    "encoding_file = 'data/BLOSUM50' # could change it to data/sparse\n",
    "\n",
    "# Random seed \n",
    "np.random.seed( 1 )\n",
    "\n",
    "# early stopping\n",
    "early_stopping = True\n",
    "\n",
    "# partition files\n",
    "partition_files = ['data/partition_3.txt', 'data/partition_2.txt', 'data/partition_6.txt', 'data/partition_5.txt', 'data/partition_4.txt']\n",
    "\n",
    "#outer and inner cv partition folds\n",
    "K1, K2 = 5, 4\n",
    "    \n",
    "# define outer partitions\n",
    "outer_partitions = assign_cv_partition(partition_files, n_folds=K1)\n",
    "for k, (outer_train, outer_test) in enumerate(outer_partitions):\n",
    "    \n",
    "    #get outer training set from outer partition for training with optimal parameters\n",
    "    outer_peptides, outer_y = data_partition(outer_train, data=data, blosum_file=encoding_file)\n",
    "    \n",
    "    #get validation set from the outer partition to validate model one\n",
    "    validation_peptides, validation_targets = data_partition(outer_test, data=data, blosum_file=encoding_file)\n",
    "    \n",
    "    # make inner partition of the training set for parameter optimsiation\n",
    "    inner_partitions = assign_cv_partition(outer_train, n_folds=K2)\n",
    "    \n",
    "    \n",
    "    #Create storage for predictions for concatenation of predictions vs targets\n",
    "    storage_pred = make_storage(lamb_list,epsilon_list)\n",
    "    storage_target = make_storage(lamb_list,epsilon_list)\n",
    "    \n",
    "    for j, (inner_train, inner_test) in enumerate(inner_partitions):\n",
    "\n",
    "        # peptides for training\n",
    "        peptides, y = data_partition(outer_train, data=data, blosum_file=encoding_file)\n",
    "        N = len(peptides)\n",
    "        \n",
    "        \n",
    "        # evaluation peptide values\n",
    "        evaluation_peptides, evaluation_targets = data_partition(outer_test, data=data, blosum_file=encoding_file)\n",
    "        \n",
    "\n",
    "        #for each lambda and epsilon combi\n",
    "        for l in lamb_list:\n",
    "            for epsilon in epsilon_list:\n",
    "                lamb_N = l/N\n",
    "                stopping_error = np.inf # error for early stopping\n",
    "                # weights\n",
    "                input_dim  = len(peptides[0])\n",
    "                w_bound = 0.1\n",
    "                weights = np.random.uniform(-w_bound, w_bound, size=input_dim)                \n",
    "\n",
    "\n",
    "                # for each training epoch\n",
    "                for e in range(0, epochs):\n",
    "\n",
    "                    # for each peptide\n",
    "                    for i in range(0, N):\n",
    "\n",
    "                        # random index\n",
    "                        ix = np.random.randint(0, N)\n",
    "\n",
    "                        # get peptide       \n",
    "                        peptide = peptides[ix]\n",
    "\n",
    "                        # get target prediction value\n",
    "                        y_target = y[ix]\n",
    "                        #print(y_target)\n",
    "                        # get initial prediction\n",
    "                        y_pred = np.dot(peptide, weights)\n",
    "                        #print(y_pred)\n",
    "                        # gradient descent \n",
    "                        gradient_descent(y_pred, y_target, peptide, weights, lamb_N, epsilon) # updates weights\n",
    "\n",
    "                    #compute error, needed for plot\n",
    "                    #gerr, mse = cumulative_error(peptides, y, l, weights) \n",
    "\n",
    "                    # predict on training data, for plots\n",
    "                    #train_pred = predict( peptides, weights )\n",
    "                    #train_mse = cal_mse( y, train_pred )\n",
    "                    #train_pcc = pearsonr( y, train_pred )\n",
    "\n",
    "                    # predict on evaluation data, needed for early stopping\n",
    "                    eval_pred = predict(evaluation_peptides, weights )\n",
    "                    eval_mse = cal_mse(evaluation_targets, eval_pred )\n",
    "                    #eval_pcc = pearsonr(evaluation_targets, eval_pred) not needed here\n",
    "\n",
    "                    # early stopping\n",
    "                    if early_stopping:\n",
    "\n",
    "                        if eval_mse < stopping_error:\n",
    "\n",
    "                            stopping_error = eval_mse # save to compare future loops\n",
    "                            #stopping_pcc = eval_pcc[0] # save to compare with best pcc\n",
    "                            stopping_pred = eval_pred[:] # will this create an alias? add slice\n",
    "\n",
    "                            #print (\"# Save network\", e, \"Best MSE\", stopping_error, \"PCC\", stopping_pcc)\n",
    "                    \n",
    "                    #Wrong way finding best lambda\n",
    "                    #if stopping_pcc > best_pcc:\n",
    "                    #    best_pcc = stopping_pcc\n",
    "                    #    best_lamb = l\n",
    "                    #    best_epsilon = epsilon\n",
    "                    \n",
    "                    #print(\"Epoch: \", e, \"Gerr:\", gerr, train_pcc[0], train_mse, eval_pcc[0], eval_mse)\n",
    "                #print(\"Lambda \", l, \"Epsilon \",epsilon,\"PCC \",stopping_pcc, \"Outer \",k, \"Inner\",j)\n",
    "                \n",
    "                #save hyperparameter iteration to concatenated prediction vs target objects\n",
    "                storage_pred[l][epsilon].extend(stopping_pred)\n",
    "                storage_target[l][epsilon].extend(evaluation_targets)\n",
    "                \n",
    "                #print(storage_matrix)\n",
    "        \n",
    "    #Select best model from inner cv loop\n",
    "    best_pcc = -np.inf  #we are not training towards negative correlation\n",
    "    \n",
    "    #calculate pcc for concatenated predictions, save and print the best one\n",
    "    for ll in lamb_list:\n",
    "        for el in epsilon_list:\n",
    "            concat_pcc = pearsonr(storage_target[ll][el], storage_pred[ll][el])[0]\n",
    "            #print(ll,el,concat_pcc)\n",
    "            if concat_pcc > best_pcc:\n",
    "                best_pcc = concat_pcc\n",
    "                best_lamb = ll\n",
    "                best_epsilon = el\n",
    "    print(\"Best PCC \", best_pcc, \"Best Lamba \", best_lamb,\"Best epsilon \",best_epsilon)\n",
    "    \n",
    "    # train on outer test set\n",
    "    N = len(outer_peptides)\n",
    "    lamb=best_lamb\n",
    "    lamb_N = lamb/N\n",
    "    epsilon=best_epsilon\n",
    "    stopping_error = np.inf # for early stopping\n",
    "    # weights\n",
    "    input_dim  = len(outer_peptides[0])\n",
    "    w_bound = 0.1\n",
    "    weights = np.random.uniform(-w_bound, w_bound, size=input_dim)\n",
    "    best_weights = np.zeros(input_dim)\n",
    "                \n",
    "    # for each training epoch\n",
    "    for e in range(0, epochs):\n",
    "\n",
    "        # for each peptide\n",
    "        for i in range(0, N):\n",
    "\n",
    "            # random index\n",
    "            ix = np.random.randint(0, N)\n",
    "\n",
    "            # get peptide       \n",
    "            peptide = outer_peptides[ix]\n",
    "\n",
    "            # get target prediction value\n",
    "            y_target = outer_y[ix]\n",
    "            #print(y_target)\n",
    "            # get initial prediction\n",
    "            y_pred = np.dot(peptide, weights)\n",
    "            #print(y_pred)\n",
    "            # gradient descent \n",
    "            gradient_descent(y_pred, y_target, peptide, weights, lamb_N, epsilon) # updates weights\n",
    "\n",
    "        #compute error, needed for plot\n",
    "        #gerr, mse = cumulative_error(outer_peptides, outer_y, lamb, weights) \n",
    "\n",
    "        # predict on training data, only needed for plots\n",
    "        #train_pred = predict( outer_peptides, weights )\n",
    "        #train_mse = cal_mse( outer_y, train_pred )\n",
    "        #train_pcc = pearsonr( outer_y, train_pred )\n",
    "\n",
    "        # predict on outer test (validation data)\n",
    "        eval_pred = predict(validation_peptides, weights )\n",
    "        eval_mse = cal_mse(validation_targets, eval_pred )\n",
    "        #eval_pcc = pearsonr(validation_targets, eval_pred) don't calculate it unless needed\n",
    "\n",
    "        # early stopping\n",
    "        if early_stopping:\n",
    "\n",
    "            if eval_mse < stopping_error:\n",
    "\n",
    "                stopping_error = eval_mse # save to compare future loops\n",
    "                stopping_pcc = pearsonr(validation_targets, eval_pred)[0]\n",
    "                best_weights = weights[:]\n",
    "\n",
    "    print(\"Lambda: \", lamb,\"Epsilon: \", epsilon, \"PCC: \", stopping_pcc)\n",
    "    print(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(storage_pred[10][0.0005]))\n",
    "#print(len(storage_target[10][0.0005]))\n",
    "#print(storage_pred[10][0.0005])\n",
    "#print(storage_target[10][0.0005])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "smm_gradient_descent_v2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
