{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7kEGGMRKQBdl"
   },
   "source": [
    "# SMM with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RDGR-iaV3vf1"
   },
   "source": [
    "## Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlFGxFZ997P4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4zgNC-G3xwY"
   },
   "source": [
    "## Data Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE THE PATH TO YOUR COURSE DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hannah's partition scheme and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_blosum(filename):\n",
    "    \"\"\"\n",
    "    Read in BLOSUM values into matrix.\n",
    "    \"\"\"\n",
    "    aa = ['A', 'R', 'N' ,'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V', 'X']\n",
    "    df = pd.read_csv(filename, sep=' ', comment='#', index_col=0)\n",
    "    return df.loc[aa, aa]\n",
    "\n",
    "def load_peptide_target(filename, MAX_PEP_SEQ_LEN=9):\n",
    "    \"\"\"\n",
    "    Read amino acid sequence of peptides and\n",
    "    corresponding log transformed IC50 binding values from text file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename, sep=' ', usecols=[0,1], names=['peptide','target'])\n",
    "    return df[df.peptide.apply(len) <= MAX_PEP_SEQ_LEN]\n",
    "\n",
    "def load_pickle(f):\n",
    "    with open(f, 'rb') as source:\n",
    "        s = pickle.load(source)\n",
    "    return s\n",
    "\n",
    "def load_partitions(files):\n",
    "    o = []\n",
    "    for f in files:\n",
    "        data = load_pickle(f)\n",
    "        o += data\n",
    "    return o\n",
    "\n",
    "def assign_cv_partition(partition_files, n_folds=5, n_test=1):\n",
    "    \"\"\"Figure out all combinations of partition_files to assign as train and test data in CV\"\"\"\n",
    "\n",
    "    # how many combinations of partition_files in train part\n",
    "    n_train = n_folds - n_test\n",
    "\n",
    "    # find all combinations of the partition_files with n_train files in each\n",
    "    train_files = list(combinations(partition_files, n_train))\n",
    "\n",
    "    # convert each list element to tuple so (train_partitions, test_partition)\n",
    "    files = [\n",
    "        (x, list(set(partition_files) - set(x))) for x in train_files\n",
    "    ]\n",
    "\n",
    "    return files\n",
    "\n",
    "def data_partition(partition_files, data, blosum_file, batch_size=32, n_features=9):\n",
    "    partitions = load_partitions(partition_files)\n",
    "\n",
    "    selected_data = data.loc[data.peptide.isin(partitions), ].reset_index()\n",
    "\n",
    "    X, y = encode_peptides(selected_data, blosum_file=blosum_file, batch_size=batch_size, n_features=n_features)\n",
    "\n",
    "    # reshape X\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def encode_peptides(Xin, blosum_file, batch_size, n_features, MAX_PEP_SEQ_LEN=9):\n",
    "    \"\"\"\n",
    "    Encode AA seq of peptides using BLOSUM50.\n",
    "    Returns a tensor of encoded peptides of shape (batch_size, MAX_PEP_SEQ_LEN, n_features)\n",
    "    \"\"\"\n",
    "    blosum = load_blosum(blosum_file)\n",
    "    \n",
    "    batch_size = len(Xin)\n",
    "    n_features = len(blosum)\n",
    "    \n",
    "    Xout = np.zeros((batch_size, MAX_PEP_SEQ_LEN, n_features), dtype=np.int8) # should it be uint? is there a purpose to that?\n",
    "    \n",
    "    for peptide_index, row in Xin.iterrows():\n",
    "        for aa_index in range(len(row.peptide)):\n",
    "            Xout[peptide_index, aa_index] = blosum[ row.peptide[aa_index] ].values\n",
    "            \n",
    "    return Xout, Xin.target.values\n",
    "\n",
    "data = load_peptide_target('data/A0201/A0201.dat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uzw3jsGnzw0e"
   },
   "source": [
    "### Alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYcVU4VLzyVe"
   },
   "outputs": [],
   "source": [
    "alphabet_file = data_dir + \"Matrices/alphabet\"\n",
    "alphabet = np.loadtxt(alphabet_file, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww-p6gy81mqk"
   },
   "source": [
    "## Error Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hv2nx4Yq1lBf"
   },
   "outputs": [],
   "source": [
    "def cumulative_error(peptides, y, lamb, weights):\n",
    "\n",
    "    error = 0\n",
    "    \n",
    "    for i in range(0, len(peptides)):\n",
    "        \n",
    "        # get peptide\n",
    "        peptide = peptides[i]\n",
    "\n",
    "        # get target prediction value\n",
    "        y_target = y[i]\n",
    "        \n",
    "        # get prediction\n",
    "        y_pred = np.dot(peptide, weights)\n",
    "            \n",
    "        # calculate error\n",
    "        error += 1.0/2 * (y_pred - y_target)**2\n",
    "        \n",
    "    gerror = error + lamb*np.dot(weights, weights)\n",
    "    error /= len(peptides)\n",
    "        \n",
    "    return gerror, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict value for a peptide list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(peptides, weights):\n",
    "\n",
    "    pred = []\n",
    "    \n",
    "    for i in range(0, len(peptides)):\n",
    "        \n",
    "        # get peptide\n",
    "        peptide = peptides[i]\n",
    "        \n",
    "        # get prediction\n",
    "        y_pred = np.dot(peptide, weights)\n",
    "        \n",
    "        pred.append(y_pred)\n",
    "        \n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate MSE between two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mse(vec1, vec2):\n",
    "    \n",
    "    mse = 0\n",
    "    \n",
    "    for i in range(0, len(vec1)):\n",
    "        mse += (vec1[i] - vec2[i])**2\n",
    "        \n",
    "    mse /= len(vec1)\n",
    "    \n",
    "    return( mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfvPqSjL2g7u"
   },
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HMXHiHmE2gh9"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(y_pred, y_target, peptide, weights, lamb_N, epsilon):\n",
    "    \n",
    "    # do is dE/dO\n",
    "    #do = XX\n",
    "    do = y_pred - y_target\n",
    "    \n",
    "    for i in range(0, len(weights)):\n",
    "        \n",
    "        #de_dw_i = XX\n",
    "        de_dw_i = do * peptide[i] + 2 * lamb_N * weights[i]\n",
    "        #weights[i] -= XX\n",
    "        weights[i] -= epsilon * de_dw_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make storage for inner loop predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_storage(hyper_parameter_1, hyper_parameter_2):\n",
    "    store_predictions={}\n",
    "\n",
    "    for i in hyper_parameter_1:\n",
    "    \n",
    "        store_predictions[i]={}\n",
    "    \n",
    "        for j in hyper_parameter_2:\n",
    "        \n",
    "            store_predictions[i][j] = []\n",
    "    return(store_predictions)\n",
    "\n",
    "#storage_matrix = make_storage([\"a\",\"b\",\"c\"],[1,2,3])\n",
    "#print(storage_matrix)\n",
    "#storage_matrix[\"a\"][1].extend([1,1,1])\n",
    "#storage_matrix[\"a\"][1].extend([1,1,1])\n",
    "\n",
    "#for i in storage_matrix:\n",
    "    \n",
    "#    for j in storage_matrix[\"a\"]:\n",
    "#        print(i,j)\n",
    "#        print(storage_matrix[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHXm8HAm4S_u"
   },
   "source": [
    "## Main Loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1717
    },
    "colab_type": "code",
    "id": "EcHQYE2sja-y",
    "outputId": "3939a58c-88ac-4ae1-b680-edc4c2c913a1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best PCC  0.44840993524815387 Best Lamba  0.1 Best epsilon  0.0005\n",
      "Lambda:  0.1 Epsilon:  0.0005 PCC:  0.42186963338711225\n",
      "[-0.01441446  0.00796385 -0.01882105  0.0191119   0.0055266   0.01132746\n",
      " -0.01043916 -0.00404432  0.01482416 -0.02846233 -0.01653393  0.0027511\n",
      " -0.01272716  0.05308011  0.00865423  0.02166076 -0.01058245  0.00731797\n",
      "  0.02500993  0.03343337 -0.02405739  0.05428056 -0.04674048  0.01966836\n",
      " -0.01049091 -0.00794497 -0.03883371 -0.10501184 -0.05428164 -0.02062942\n",
      " -0.04822023 -0.0529048   0.02349893 -0.02719188  0.06761099 -0.03660979\n",
      " -0.05145079 -0.04565713 -0.01415729 -0.07012812  0.02149843  0.02832422\n",
      " -0.04546773 -0.04396654  0.00900083  0.01194521 -0.01183724  0.03165416\n",
      "  0.05736613 -0.01035097  0.05854466  0.02246981 -0.03102757 -0.04225283\n",
      " -0.0046886  -0.09056699 -0.00856822  0.03265908  0.05694171 -0.03537934\n",
      "  0.06153646  0.03535961  0.07496989  0.00173663 -0.00745969  0.00458081\n",
      " -0.02400088 -0.00422472  0.00103082 -0.02420956  0.00461926 -0.00230777\n",
      "  0.01934962 -0.00398051  0.04226567  0.00193839 -0.02384421  0.00895878\n",
      "  0.00975087  0.01697024  0.0047502  -0.0170785   0.00498494  0.04366363\n",
      " -0.0380927  -0.00911072  0.00669342 -0.0230633  -0.00270923 -0.01301242\n",
      " -0.04906777  0.01852011 -0.01350404  0.00878451  0.01133586  0.03267156\n",
      " -0.00285381 -0.01456487 -0.02198172 -0.03974054 -0.01163105  0.00724164\n",
      " -0.0010685  -0.02015631  0.02153323  0.04791525  0.00073937 -0.04511183\n",
      "  0.03834904  0.00106912  0.03498467  0.01432021 -0.02734395 -0.03059203\n",
      " -0.00279187  0.01879629  0.02599272 -0.0039835   0.00969222  0.01004392\n",
      "  0.02819861 -0.02723327  0.00159922 -0.06440779  0.01680109 -0.01439564\n",
      " -0.06517687 -0.01553282 -0.03287911  0.02476359 -0.00720352  0.02085745\n",
      "  0.04573721 -0.02225117  0.02428898  0.05358095 -0.04794748 -0.02791084\n",
      " -0.03716672 -0.01936923 -0.02096509 -0.00012974 -0.0144712   0.00888674\n",
      "  0.07717455 -0.02538881 -0.00826757  0.01109411 -0.00534974  0.00502071\n",
      " -0.03655585  0.00434482 -0.00209602 -0.02347847  0.04530631  0.04135247\n",
      " -0.0406718  -0.03527174 -0.00544365 -0.01269381 -0.0108086   0.01894642\n",
      "  0.02110399 -0.00033839 -0.0038882   0.03353505  0.03219693  0.00711767\n",
      "  0.04854256 -0.04432487 -0.02870717 -0.04300665 -0.03600262  0.03023263\n",
      "  0.04579771  0.04409714  0.0236508   0.04929787  0.02518478  0.01006542\n",
      "  0.00825149 -0.07216858 -0.03176375 -0.04033984 -0.00285717 -0.00659873\n",
      " -0.01256358 -0.07992624  0.06733894]\n",
      "Best PCC  0.48776305579492707 Best Lamba  0.01 Best epsilon  0.0005\n",
      "Lambda:  0.01 Epsilon:  0.0005 PCC:  0.5224465119646807\n",
      "[-0.07038739  0.0600525  -0.01675434  0.06912795  0.02095368 -0.02041572\n",
      "  0.00433665 -0.05696235 -0.05080545  0.01171549  0.0415478  -0.02710433\n",
      " -0.00226162  0.05073332 -0.00608555  0.0867862  -0.01218933  0.04754599\n",
      " -0.01622453 -0.02544047 -0.00798501 -0.05636601 -0.02567712  0.00495384\n",
      " -0.02200695  0.00230203 -0.03391113 -0.05763696 -0.00994398 -0.01042847\n",
      "  0.0090588   0.02320835  0.02068433 -0.00211434 -0.03444627 -0.005848\n",
      " -0.01987333 -0.00703284  0.01923329  0.05648959 -0.07036558 -0.05170716\n",
      " -0.09816236  0.00329456  0.00250304  0.03360276 -0.01073052  0.02435725\n",
      "  0.01796841 -0.04621463  0.01602362  0.04366041 -0.04549869 -0.01495943\n",
      " -0.03647712  0.02155434 -0.00874354  0.00846666  0.0005294   0.01526361\n",
      "  0.07141852 -0.01434339  0.00221137 -0.0122644  -0.00307629 -0.04942337\n",
      " -0.06318566 -0.00260111  0.01726344 -0.04742203  0.03891952  0.01865801\n",
      " -0.02761677  0.00478392  0.01297213 -0.03626924 -0.0019448  -0.00439152\n",
      " -0.02876227 -0.02800618 -0.00392104  0.00591366 -0.02670469  0.01771893\n",
      "  0.03579539 -0.01017565 -0.01334569  0.05287344 -0.00325119  0.0058773\n",
      "  0.02983554 -0.050271   -0.04055167  0.02402257  0.02269709 -0.00337303\n",
      "  0.0194875   0.022816   -0.01442382  0.00608346 -0.02753901  0.01585683\n",
      " -0.05707037 -0.01246837 -0.01954821 -0.00051599  0.0148404  -0.00944048\n",
      " -0.02545617 -0.01003086  0.00856979 -0.03257442  0.04064565 -0.023333\n",
      " -0.06361     0.06226485  0.03731892  0.00597652  0.06847758 -0.00983827\n",
      " -0.00714189  0.01366982  0.0226574  -0.04385514 -0.0502575   0.05133548\n",
      " -0.01307758  0.03389498 -0.03237557  0.00339474 -0.00509634 -0.01177501\n",
      " -0.00904583  0.00568888  0.02635345  0.05645794 -0.04914524 -0.05094833\n",
      " -0.05411635 -0.02019893  0.00376329  0.06660248 -0.02091617  0.02597533\n",
      "  0.0283373  -0.03898868  0.02946057 -0.03884239 -0.01144085  0.01312427\n",
      " -0.02821607 -0.00265042  0.00588222  0.02091593  0.00751737  0.05167508\n",
      "  0.04305649 -0.04041725 -0.02136411 -0.00441254 -0.04008557  0.030636\n",
      "  0.02438663  0.03470316 -0.01674241  0.02546143  0.03441116  0.06869935\n",
      "  0.01654249 -0.01644866  0.00839257  0.07741523 -0.00462966  0.04087808\n",
      "  0.02426259 -0.02188463 -0.03456757 -0.04114956 -0.03052894  0.04088653\n",
      "  0.02343689  0.07891308  0.02508863  0.00887129  0.04841659 -0.0243615\n",
      "  0.07099464 -0.01622928 -0.02187117]\n",
      "Best PCC  0.5120523060370735 Best Lamba  1 Best epsilon  0.0005\n",
      "Lambda:  1 Epsilon:  0.0005 PCC:  0.49012616393626113\n",
      "[-3.16108420e-02  4.34073119e-02  5.24443916e-02 -1.33063958e-02\n",
      " -1.54665696e-02 -6.56699807e-02 -5.62841345e-03  9.58865317e-02\n",
      "  6.81768453e-03 -6.82632643e-02  3.46123929e-02 -6.59837646e-02\n",
      "  4.36463805e-02 -1.96800002e-02 -1.44536321e-02  2.35440083e-02\n",
      "  4.53904210e-02 -9.20661023e-03 -5.78471379e-03 -1.40927226e-02\n",
      " -4.45202088e-02 -7.35796410e-02 -1.99528258e-02 -4.61660853e-02\n",
      " -1.11854037e-02  7.90878877e-03  2.32418398e-02 -1.80885921e-02\n",
      " -1.67488385e-02 -2.99508344e-03 -4.22988325e-02  4.14513753e-02\n",
      "  1.62419359e-02 -1.21914493e-02 -3.40280592e-02  2.57421409e-02\n",
      "  2.46382206e-02  7.80223800e-03 -1.16736796e-02  2.29297351e-02\n",
      "  1.06210679e-02  6.75520686e-02 -7.31017433e-03  5.92530715e-02\n",
      "  9.80322595e-02  4.20068815e-02 -1.70869631e-02 -5.95824851e-02\n",
      "  2.51028476e-02  1.58114135e-02 -3.06266198e-02 -2.55314123e-02\n",
      "  2.60782475e-02 -4.83033068e-02  2.82809501e-02  4.16523545e-02\n",
      " -2.55269001e-02  3.84295079e-02  1.04500808e-01 -2.40635016e-03\n",
      " -1.30870477e-03 -4.38091150e-02 -1.03042200e-02  1.55265506e-02\n",
      "  5.60157191e-02 -6.20115772e-02 -2.74043430e-02  1.00724640e-02\n",
      " -1.14501669e-03 -5.94066750e-02  4.56588895e-02 -2.51256848e-02\n",
      " -6.21682872e-02  3.99222434e-02  3.14677706e-02 -3.13300989e-02\n",
      "  6.92016057e-02  4.50161063e-03 -1.26385521e-02 -6.08585141e-02\n",
      "  4.02851737e-02 -3.84031174e-02 -4.92563482e-02 -3.84903755e-02\n",
      " -2.99812483e-02  3.11220006e-02  1.25148486e-02 -3.30724789e-02\n",
      " -1.58684321e-03 -4.31602064e-02 -3.14796514e-02  3.98098179e-02\n",
      " -1.69852521e-02  1.50350837e-02 -2.68871880e-02  6.52755860e-03\n",
      " -2.10511349e-02  6.99628548e-02 -2.50303578e-02 -4.22117170e-02\n",
      "  4.73065705e-02 -3.60687802e-04 -4.13570409e-03 -3.54455877e-02\n",
      "  1.67570822e-02  2.93901512e-02 -4.61757726e-02  2.10094598e-02\n",
      " -1.85355225e-02 -2.33540459e-03 -5.56038719e-02 -2.97542117e-02\n",
      " -1.26093019e-02 -3.53162169e-03 -6.15850565e-05 -9.53356732e-03\n",
      " -1.11532113e-02 -1.70806035e-02 -5.46705882e-02 -1.37468533e-02\n",
      " -1.60106353e-02 -8.67026395e-03 -7.97154405e-03 -3.76819182e-03\n",
      "  3.95079061e-02  6.49807674e-03 -6.63460235e-02  6.59421909e-03\n",
      " -6.04829185e-03 -7.47426621e-02 -1.03433401e-02  2.19536814e-03\n",
      "  9.64417454e-03  6.37528161e-02  6.92273801e-02 -2.18759096e-02\n",
      " -5.42289616e-02 -3.28935927e-02 -3.56871054e-02 -2.98200475e-02\n",
      "  1.36505744e-02  3.88360646e-03  4.83342537e-02 -1.33027883e-02\n",
      "  8.18659027e-02  1.36805067e-02  4.10237173e-02  3.45861726e-03\n",
      "  1.73094295e-02 -1.69742096e-02  3.09299961e-02 -1.52928975e-02\n",
      "  3.75350166e-02  4.51549917e-02 -2.69394703e-03 -1.04292903e-02\n",
      "  3.84783854e-03  2.17127440e-02 -2.05929088e-02  3.32349989e-02\n",
      " -5.68001152e-03  1.06612228e-02  3.47278420e-02  2.85877486e-02\n",
      " -2.05527779e-02 -1.62478296e-02 -1.20081605e-02 -3.84044391e-02\n",
      "  7.91057611e-02  4.17711970e-02  5.74036603e-02  3.14404399e-02\n",
      " -3.80180753e-02 -4.41074813e-02 -3.84486406e-02  2.45393088e-02\n",
      " -1.78400425e-02  4.06066204e-03 -3.30317481e-02 -9.01439336e-03\n",
      " -1.70972036e-02  6.23460894e-02 -1.88605641e-02  2.93118492e-02\n",
      " -3.01843601e-03  2.76589095e-02 -2.97206608e-02 -2.95174604e-02\n",
      "  2.07508372e-02]\n",
      "Best PCC  0.5026395857455495 Best Lamba  1 Best epsilon  0.0005\n",
      "Lambda:  1 Epsilon:  0.0005 PCC:  0.45519737707328956\n",
      "[-6.34814840e-02  1.60808650e-02  3.96256552e-02  1.48078456e-02\n",
      " -2.20948809e-03 -3.32689129e-02 -1.85800161e-02  1.15980044e-02\n",
      " -1.28228904e-02  1.60577067e-02  7.50813851e-03 -4.38924067e-02\n",
      "  5.33051840e-03 -4.94721300e-02 -1.06425225e-02  5.07856553e-02\n",
      "  4.02870061e-02 -6.61909055e-03  1.81885171e-02  1.01841957e-02\n",
      "  1.38095222e-02 -4.16420777e-02 -8.88712176e-03  2.19886357e-02\n",
      "  6.57428600e-02 -1.53026686e-02 -5.01162557e-02 -6.19574142e-02\n",
      "  1.45529326e-02 -4.27924078e-02 -5.34437581e-02 -9.60458168e-03\n",
      " -3.62486122e-03 -3.23557547e-02  6.27826236e-02 -4.60029621e-03\n",
      "  1.69665254e-02 -2.31740630e-02  1.07244773e-02  7.37507757e-03\n",
      "  2.53133278e-02 -8.53135890e-02  1.46816031e-04 -2.01011950e-02\n",
      "  6.59939363e-03  3.18089196e-02 -6.02749978e-03  2.42501696e-02\n",
      "  1.62204885e-02 -1.82334813e-02 -5.68528688e-04  1.41442613e-02\n",
      " -1.57911982e-02  1.27561921e-02  1.53010551e-02 -1.43654614e-02\n",
      "  5.03009791e-03  3.42354419e-02 -9.13740541e-04  4.93902909e-03\n",
      "  6.07004098e-03  2.07283114e-02 -9.42541525e-03  5.03140359e-02\n",
      "  1.03226118e-02 -6.68098625e-02 -1.44796427e-02  2.06696413e-03\n",
      "  3.53091926e-02 -8.26287035e-03  7.41344141e-03  1.89577600e-02\n",
      "  1.26211536e-03  1.71427393e-02  1.10484500e-02 -2.02013468e-02\n",
      " -8.96088800e-02  3.02540643e-02  4.42017725e-02 -5.91951586e-02\n",
      " -1.11304113e-03 -3.13978249e-02  3.64706817e-02 -4.39511066e-02\n",
      " -6.54844743e-03 -6.76237234e-03  1.86526329e-06 -4.29047144e-02\n",
      " -1.15853404e-02 -7.64126327e-03 -2.11756882e-02  5.06061377e-02\n",
      "  4.81796253e-02 -2.21970906e-02 -1.16051387e-02  2.53231850e-03\n",
      " -2.62820770e-03 -7.05155881e-02  1.36409209e-02  2.37763176e-02\n",
      "  8.17759651e-03 -5.84917479e-03  5.87811451e-03  2.47731394e-02\n",
      " -1.23363562e-02  6.13997065e-03 -1.08111606e-02  4.13418029e-02\n",
      "  1.12512196e-02 -1.76006962e-02 -4.45422738e-02 -2.74560576e-02\n",
      "  2.74284459e-02  2.82055297e-02 -4.65862767e-02  3.52383575e-03\n",
      " -3.99172151e-02 -1.90096681e-02 -9.10263181e-02  8.71072690e-03\n",
      "  6.76791750e-02 -1.64991498e-02 -6.12176063e-03  3.53190376e-02\n",
      "  5.40480121e-02 -5.66725849e-02 -5.26243915e-02 -2.85885898e-02\n",
      "  2.79655789e-02  2.99510326e-02 -1.25761296e-02 -1.35431475e-02\n",
      "  4.93926921e-03 -2.01479264e-02 -1.04936339e-02  1.39842545e-02\n",
      " -4.06973816e-02 -2.27374913e-02 -1.38344028e-02 -3.36342367e-02\n",
      " -1.34088572e-02 -2.03833017e-02 -2.85410384e-03 -3.94946032e-03\n",
      "  5.49735143e-02  5.00498638e-02 -2.42742559e-02 -9.77448566e-03\n",
      " -1.93773952e-02 -1.71761758e-02 -4.65393124e-02  8.47587179e-03\n",
      "  4.84461639e-02 -3.26581700e-02  4.33200031e-03 -3.98729346e-02\n",
      " -3.02459308e-02  4.82451685e-02  7.65182589e-02  2.35487459e-02\n",
      "  6.57233068e-02 -1.02042358e-02 -6.92402987e-02  1.05918138e-02\n",
      "  1.34891735e-02 -8.55636451e-02 -8.02787053e-03  6.42507069e-02\n",
      "  3.48081335e-02 -1.73545902e-02 -1.44503401e-02 -2.62535221e-02\n",
      " -2.06444715e-02  2.85957215e-02  2.93437293e-02 -7.60713045e-03\n",
      "  1.91977183e-02 -1.38456373e-03  4.31451529e-02  7.40883355e-03\n",
      " -2.79449300e-02 -1.37565416e-02 -5.52735315e-02 -2.53109084e-03\n",
      " -5.75571993e-02  4.90900915e-02 -2.59939192e-02 -5.15950868e-02\n",
      "  3.26115482e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best PCC  0.4658547535785149 Best Lamba  10 Best epsilon  0.0005\n",
      "Lambda:  10 Epsilon:  0.0005 PCC:  0.4438757380727587\n",
      "[-0.08356906  0.03051254  0.05482558  0.054194    0.01823742 -0.03637626\n",
      "  0.0261169  -0.07772674 -0.05044128  0.07219989 -0.01270149 -0.06107833\n",
      "  0.00362152 -0.00566356  0.00515721  0.06262778  0.03412653  0.01277316\n",
      " -0.03041253  0.06149721  0.00223588 -0.07484002 -0.01111444 -0.00477001\n",
      "  0.04314671 -0.02059152  0.015858    0.03678057 -0.0173594   0.01100481\n",
      " -0.0004887   0.02052479 -0.02843625 -0.01475344 -0.04437507  0.03895473\n",
      "  0.08523672  0.01877295 -0.00696127  0.05594191 -0.00102328  0.00521107\n",
      " -0.07131798 -0.0207041  -0.04816311  0.00514115  0.00775523  0.01109466\n",
      " -0.02785259 -0.00363095 -0.00824959 -0.01019145 -0.00178855  0.01500747\n",
      " -0.05264306 -0.00758589 -0.01917207 -0.04712233 -0.0325499   0.00537934\n",
      "  0.06941658 -0.00735993 -0.02204829  0.01377782  0.00043444 -0.07559996\n",
      " -0.02342109 -0.01263339  0.06402799 -0.01157985  0.02576573  0.01905444\n",
      "  0.00854136 -0.01301038  0.01526566 -0.00378255  0.00083691  0.0143527\n",
      "  0.00820593 -0.04561532  0.01743677 -0.0035611  -0.03100278 -0.00459744\n",
      " -0.04912832 -0.01304653  0.01153251  0.02802813  0.00133928  0.01951649\n",
      "  0.06157255 -0.04956936 -0.04443276  0.02198134 -0.01031683  0.01858781\n",
      "  0.02412744  0.10290466 -0.03331743 -0.08297573  0.03988424  0.00782845\n",
      " -0.01809189 -0.02419236  0.01598186  0.026601    0.01907383 -0.04590195\n",
      " -0.01261136 -0.00175606  0.00890482  0.02031174 -0.01954874  0.01932589\n",
      "  0.09165009 -0.04532156 -0.03510564 -0.03724262  0.00621217 -0.00757767\n",
      "  0.03345594  0.00421016  0.00528915 -0.02604504 -0.04267123  0.09382503\n",
      " -0.00414429  0.00804957 -0.02165504  0.02270262 -0.01920302  0.02499331\n",
      "  0.028023    0.03388975  0.01190194 -0.06715445  0.04769657 -0.03115807\n",
      " -0.00565806 -0.00489283 -0.00172541  0.02406999 -0.01621447 -0.01106644\n",
      "  0.01030725  0.02558983 -0.02270221 -0.02761312 -0.04087421 -0.06750133\n",
      "  0.0221183   0.01267091  0.06703249  0.00777918 -0.04739679 -0.02749704\n",
      "  0.01716331 -0.01025395  0.04515249 -0.02383597  0.00213592 -0.00922234\n",
      " -0.02461254 -0.05031034 -0.00527868  0.01259345  0.05788015 -0.03139643\n",
      "  0.05346782  0.00586124  0.01169119 -0.06796939 -0.01609969 -0.01642044\n",
      "  0.02304259  0.02297802 -0.03847354 -0.0117531   0.05562067 -0.01150597\n",
      "  0.05845769 -0.09430444 -0.00761888 -0.01769665  0.0206814   0.00237967\n",
      " -0.06610202 -0.02211403 -0.02258457]\n"
     ]
    }
   ],
   "source": [
    "#Lambdas\n",
    "lamb_list=[10,1,0.1,0.01,0.001]\n",
    "\n",
    "# learning rate, epsilon\n",
    "epsilon_list = [0.0005, 0.00005, 0.000005]\n",
    "\n",
    "# training epochs\n",
    "epochs = 1\n",
    "\n",
    "#Use Hannahs encoding loading\n",
    "encoding_file = 'data/BLOSUM50' # could change it to data/sparse\n",
    "\n",
    "# Random seed \n",
    "np.random.seed( 1 )\n",
    "\n",
    "# early stopping\n",
    "early_stopping = True\n",
    "\n",
    "# partition files\n",
    "partition_files = ['data/partition_3.txt', 'data/partition_2.txt', 'data/partition_6.txt', 'data/partition_5.txt', 'data/partition_4.txt']\n",
    "\n",
    "#outer and inner cv partition folds\n",
    "K1, K2 = 5, 4\n",
    "    \n",
    "# define outer partitions\n",
    "outer_partitions = assign_cv_partition(partition_files, n_folds=K1)\n",
    "for k, (outer_train, outer_test) in enumerate(outer_partitions):\n",
    "    \n",
    "    #get outer training set from outer partition for training with optimal parameters\n",
    "    outer_peptides, outer_y = data_partition(outer_train, data=data, blosum_file=encoding_file)\n",
    "    \n",
    "    #get validation set from the outer partition to validate model one\n",
    "    validation_peptides, validation_targets = data_partition(outer_test, data=data, blosum_file=encoding_file)\n",
    "    \n",
    "    # make inner partition of the training set for parameter optimsiation\n",
    "    inner_partitions = assign_cv_partition(outer_train, n_folds=K2)\n",
    "    \n",
    "    \n",
    "    #Create storage for predictions for concatenation of predictions vs targets\n",
    "    storage_pred = make_storage(lamb_list,epsilon_list)\n",
    "    storage_target = make_storage(lamb_list,epsilon_list)\n",
    "    \n",
    "    for j, (inner_train, inner_test) in enumerate(inner_partitions):\n",
    "\n",
    "        # peptides for training\n",
    "        peptides, y = data_partition(outer_train, data=data, blosum_file=encoding_file)\n",
    "        N = len(peptides)\n",
    "        \n",
    "        \n",
    "        # evaluation peptide values\n",
    "        evaluation_peptides, evaluation_targets = data_partition(outer_test, data=data, blosum_file=encoding_file)\n",
    "        \n",
    "\n",
    "        #for each lambda and epsilon combi\n",
    "        for l in lamb_list:\n",
    "            for epsilon in epsilon_list:\n",
    "                lamb_N = l/N\n",
    "                stopping_error = np.inf # error for early stopping\n",
    "                # weights\n",
    "                input_dim  = len(peptides[0])\n",
    "                w_bound = 0.1\n",
    "                weights = np.random.uniform(-w_bound, w_bound, size=input_dim)                \n",
    "\n",
    "\n",
    "                # for each training epoch\n",
    "                for e in range(0, epochs):\n",
    "\n",
    "                    # for each peptide\n",
    "                    for i in range(0, N):\n",
    "\n",
    "                        # random index\n",
    "                        ix = np.random.randint(0, N)\n",
    "\n",
    "                        # get peptide       \n",
    "                        peptide = peptides[ix]\n",
    "\n",
    "                        # get target prediction value\n",
    "                        y_target = y[ix]\n",
    "                        #print(y_target)\n",
    "                        # get initial prediction\n",
    "                        y_pred = np.dot(peptide, weights)\n",
    "                        #print(y_pred)\n",
    "                        # gradient descent \n",
    "                        gradient_descent(y_pred, y_target, peptide, weights, lamb_N, epsilon) # updates weights\n",
    "\n",
    "                    #compute error, needed for plot\n",
    "                    #gerr, mse = cumulative_error(peptides, y, l, weights) \n",
    "\n",
    "                    # predict on training data, for plots\n",
    "                    #train_pred = predict( peptides, weights )\n",
    "                    #train_mse = cal_mse( y, train_pred )\n",
    "                    #train_pcc = pearsonr( y, train_pred )\n",
    "\n",
    "                    # predict on evaluation data, needed for early stopping\n",
    "                    eval_pred = predict(evaluation_peptides, weights )\n",
    "                    eval_mse = cal_mse(evaluation_targets, eval_pred )\n",
    "                    #eval_pcc = pearsonr(evaluation_targets, eval_pred) not needed here\n",
    "\n",
    "                    # early stopping\n",
    "                    if early_stopping:\n",
    "\n",
    "                        if eval_mse < stopping_error:\n",
    "\n",
    "                            stopping_error = eval_mse # save to compare future loops\n",
    "                            #stopping_pcc = eval_pcc[0] # save to compare with best pcc\n",
    "                            stopping_pred = eval_pred[:] # will this create an alias? add slice\n",
    "\n",
    "                            #print (\"# Save network\", e, \"Best MSE\", stopping_error, \"PCC\", stopping_pcc)\n",
    "                    \n",
    "                    #Wrong way finding best lambda\n",
    "                    #if stopping_pcc > best_pcc:\n",
    "                    #    best_pcc = stopping_pcc\n",
    "                    #    best_lamb = l\n",
    "                    #    best_epsilon = epsilon\n",
    "                    \n",
    "                    #print(\"Epoch: \", e, \"Gerr:\", gerr, train_pcc[0], train_mse, eval_pcc[0], eval_mse)\n",
    "                #print(\"Lambda \", l, \"Epsilon \",epsilon,\"PCC \",stopping_pcc, \"Outer \",k, \"Inner\",j)\n",
    "                \n",
    "                #save hyperparameter iteration to concatenated prediction vs target objects\n",
    "                storage_pred[l][epsilon].extend(stopping_pred)\n",
    "                storage_target[l][epsilon].extend(evaluation_targets)\n",
    "                \n",
    "                #print(storage_matrix)\n",
    "        \n",
    "    #Select best model from inner cv loop\n",
    "    best_pcc = -np.inf  #we are not training towards negative correlation\n",
    "    \n",
    "    #calculate pcc for concatenated predictions, save and print the best one\n",
    "    for ll in lamb_list:\n",
    "        for el in epsilon_list:\n",
    "            concat_pcc = pearsonr(storage_target[ll][el], storage_pred[ll][el])[0]\n",
    "            #print(ll,el,concat_pcc)\n",
    "            if concat_pcc > best_pcc:\n",
    "                best_pcc = concat_pcc\n",
    "                best_lamb = ll\n",
    "                best_epsilon = el\n",
    "    print(\"Best PCC \", best_pcc, \"Best Lamba \", best_lamb,\"Best epsilon \",best_epsilon)\n",
    "    \n",
    "    # train on outer test set\n",
    "    N = len(outer_peptides)\n",
    "    lamb=best_lamb\n",
    "    lamb_N = lamb/N\n",
    "    epsilon=best_epsilon\n",
    "    stopping_error = np.inf # for early stopping\n",
    "    # weights\n",
    "    input_dim  = len(outer_peptides[0])\n",
    "    w_bound = 0.1\n",
    "    weights = np.random.uniform(-w_bound, w_bound, size=input_dim)\n",
    "    best_weights = np.zeros(input_dim)\n",
    "                \n",
    "    # for each training epoch\n",
    "    for e in range(0, epochs):\n",
    "\n",
    "        # for each peptide\n",
    "        for i in range(0, N):\n",
    "\n",
    "            # random index\n",
    "            ix = np.random.randint(0, N)\n",
    "\n",
    "            # get peptide       \n",
    "            peptide = outer_peptides[ix]\n",
    "\n",
    "            # get target prediction value\n",
    "            y_target = outer_y[ix]\n",
    "            #print(y_target)\n",
    "            # get initial prediction\n",
    "            y_pred = np.dot(peptide, weights)\n",
    "            #print(y_pred)\n",
    "            # gradient descent \n",
    "            gradient_descent(y_pred, y_target, peptide, weights, lamb_N, epsilon) # updates weights\n",
    "\n",
    "        #compute error, needed for plot\n",
    "        #gerr, mse = cumulative_error(outer_peptides, outer_y, lamb, weights) \n",
    "\n",
    "        # predict on training data, only needed for plots\n",
    "        #train_pred = predict( outer_peptides, weights )\n",
    "        #train_mse = cal_mse( outer_y, train_pred )\n",
    "        #train_pcc = pearsonr( outer_y, train_pred )\n",
    "\n",
    "        # predict on outer test (validation data)\n",
    "        eval_pred = predict(validation_peptides, weights )\n",
    "        eval_mse = cal_mse(validation_targets, eval_pred )\n",
    "        #eval_pcc = pearsonr(validation_targets, eval_pred) don't calculate it unless needed\n",
    "\n",
    "        # early stopping\n",
    "        if early_stopping:\n",
    "\n",
    "            if eval_mse < stopping_error:\n",
    "\n",
    "                stopping_error = eval_mse # save to compare future loops\n",
    "                stopping_pcc = pearsonr(validation_targets, eval_pred)[0]\n",
    "                best_weights = weights[:]\n",
    "\n",
    "    print(\"Lambda: \", lamb,\"Epsilon: \", epsilon, \"PCC: \", stopping_pcc)\n",
    "    print(best_weights)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "smm_gradient_descent_v2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
