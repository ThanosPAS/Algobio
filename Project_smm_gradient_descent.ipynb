{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7kEGGMRKQBdl"
   },
   "source": [
    "# SMM with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RDGR-iaV3vf1"
   },
   "source": [
    "## Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlFGxFZ997P4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4zgNC-G3xwY"
   },
   "source": [
    "## Data Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE THE PATH TO YOUR COURSE DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hannah's partition scheme and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_blosum(filename):\n",
    "    \"\"\"\n",
    "    Read in BLOSUM values into matrix.\n",
    "    \"\"\"\n",
    "    aa = ['A', 'R', 'N' ,'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V', 'X']\n",
    "    df = pd.read_csv(filename, sep=' ', comment='#', index_col=0)\n",
    "    return df.loc[aa, aa]\n",
    "\n",
    "def load_peptide_target(filename, MAX_PEP_SEQ_LEN=9):\n",
    "    \"\"\"\n",
    "    Read amino acid sequence of peptides and\n",
    "    corresponding log transformed IC50 binding values from text file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename, sep=' ', usecols=[0,1], names=['peptide','target'])\n",
    "    return df[df.peptide.apply(len) <= MAX_PEP_SEQ_LEN]\n",
    "\n",
    "def load_pickle(f):\n",
    "    with open(f, 'rb') as source:\n",
    "        s = pickle.load(source)\n",
    "    return s\n",
    "\n",
    "def load_partitions(files):\n",
    "    o = []\n",
    "    for f in files:\n",
    "        data = load_pickle(f)\n",
    "        o += data\n",
    "    return o\n",
    "\n",
    "def assign_cv_partition(partition_files, n_folds=5, n_test=1):\n",
    "    \"\"\"Figure out all combinations of partition_files to assign as train and test data in CV\"\"\"\n",
    "\n",
    "    # how many combinations of partition_files in train part\n",
    "    n_train = n_folds - n_test\n",
    "\n",
    "    # find all combinations of the partition_files with n_train files in each\n",
    "    train_files = list(combinations(partition_files, n_train))\n",
    "\n",
    "    # convert each list element to tuple so (train_partitions, test_partition)\n",
    "    files = [\n",
    "        (x, list(set(partition_files) - set(x))) for x in train_files\n",
    "    ]\n",
    "\n",
    "    return files\n",
    "\n",
    "def data_partition(partition_files, data, blosum_file, batch_size=32, n_features=9):\n",
    "    partitions = load_partitions(partition_files)\n",
    "\n",
    "    selected_data = data.loc[data.peptide.isin(partitions), ].reset_index()\n",
    "\n",
    "    X, y = encode_peptides(selected_data, blosum_file=blosum_file, batch_size=batch_size, n_features=n_features)\n",
    "\n",
    "    # reshape X\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def encode_peptides(Xin, blosum_file, batch_size, n_features, MAX_PEP_SEQ_LEN=9):\n",
    "    \"\"\"\n",
    "    Encode AA seq of peptides using BLOSUM50.\n",
    "    Returns a tensor of encoded peptides of shape (batch_size, MAX_PEP_SEQ_LEN, n_features)\n",
    "    \"\"\"\n",
    "    blosum = load_blosum(blosum_file)\n",
    "    \n",
    "    batch_size = len(Xin)\n",
    "    n_features = len(blosum)\n",
    "    \n",
    "    Xout = np.zeros((batch_size, MAX_PEP_SEQ_LEN, n_features), dtype=np.int8) # should it be uint? is there a purpose to that?\n",
    "    \n",
    "    for peptide_index, row in Xin.iterrows():\n",
    "        for aa_index in range(len(row.peptide)):\n",
    "            Xout[peptide_index, aa_index] = blosum[ row.peptide[aa_index] ].values\n",
    "            \n",
    "    return Xout, Xin.target.values\n",
    "\n",
    "data = load_peptide_target('data/A0201/A0201.dat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uzw3jsGnzw0e"
   },
   "source": [
    "### Alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYcVU4VLzyVe"
   },
   "outputs": [],
   "source": [
    "alphabet_file = data_dir + \"Matrices/alphabet\"\n",
    "alphabet = np.loadtxt(alphabet_file, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww-p6gy81mqk"
   },
   "source": [
    "## Error Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hv2nx4Yq1lBf"
   },
   "outputs": [],
   "source": [
    "def cumulative_error(peptides, y, lamb, weights):\n",
    "\n",
    "    error = 0\n",
    "    \n",
    "    for i in range(0, len(peptides)):\n",
    "        \n",
    "        # get peptide\n",
    "        peptide = peptides[i]\n",
    "\n",
    "        # get target prediction value\n",
    "        y_target = y[i]\n",
    "        \n",
    "        # get prediction\n",
    "        y_pred = np.dot(peptide, weights)\n",
    "            \n",
    "        # calculate error\n",
    "        error += 1.0/2 * (y_pred - y_target)**2\n",
    "        \n",
    "    gerror = error + lamb*np.dot(weights, weights)\n",
    "    error /= len(peptides)\n",
    "        \n",
    "    return gerror, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict value for a peptide list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(peptides, weights):\n",
    "\n",
    "    pred = []\n",
    "    \n",
    "    for i in range(0, len(peptides)):\n",
    "        \n",
    "        # get peptide\n",
    "        peptide = peptides[i]\n",
    "        \n",
    "        # get prediction\n",
    "        y_pred = np.dot(peptide, weights)\n",
    "        \n",
    "        pred.append(y_pred)\n",
    "        \n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate MSE between two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mse(vec1, vec2):\n",
    "    \n",
    "    mse = 0\n",
    "    \n",
    "    for i in range(0, len(vec1)):\n",
    "        mse += (vec1[i] - vec2[i])**2\n",
    "        \n",
    "    mse /= len(vec1)\n",
    "    \n",
    "    return( mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfvPqSjL2g7u"
   },
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HMXHiHmE2gh9"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(y_pred, y_target, peptide, weights, lamb_N, epsilon):\n",
    "    \n",
    "    # do is dE/dO\n",
    "    #do = XX\n",
    "    do = y_pred - y_target\n",
    "    \n",
    "    for i in range(0, len(weights)):\n",
    "        \n",
    "        #de_dw_i = XX\n",
    "        de_dw_i = do * peptide[i] + 2 * lamb_N * weights[i]\n",
    "        #weights[i] -= XX\n",
    "        weights[i] -= epsilon * de_dw_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make storage for inner loop predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_storage(hyper_parameter_1, hyper_parameter_2):\n",
    "    store_predictions={}\n",
    "\n",
    "    for i in hyper_parameter_1:\n",
    "    \n",
    "        store_predictions[i]={}\n",
    "    \n",
    "        for j in hyper_parameter_2:\n",
    "        \n",
    "            store_predictions[i][j] = []\n",
    "    return(store_predictions)\n",
    "\n",
    "#storage_matrix = make_storage([\"a\",\"b\",\"c\"],[1,2,3])\n",
    "#print(storage_matrix)\n",
    "#storage_matrix[\"a\"][1].extend([1,1,1])\n",
    "#storage_matrix[\"a\"][1].extend([1,1,1])\n",
    "\n",
    "#for i in storage_matrix:\n",
    "    \n",
    "#    for j in storage_matrix[\"a\"]:\n",
    "#        print(i,j)\n",
    "#        print(storage_matrix[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHXm8HAm4S_u"
   },
   "source": [
    "## Main Loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1717
    },
    "colab_type": "code",
    "id": "EcHQYE2sja-y",
    "outputId": "3939a58c-88ac-4ae1-b680-edc4c2c913a1",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-2b54cd399add>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                     \u001b[1;31m# predict on evaluation data, needed for early stopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m                     \u001b[0meval_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluation_peptides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m                     \u001b[0meval_mse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcal_mse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluation_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_pred\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                     \u001b[1;31m#eval_pcc = pearsonr(evaluation_targets, eval_pred) not needed here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-97907137e722>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(peptides, weights)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# get prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeptide\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Lambdas\n",
    "lamb_list=[10,1,0.1,0.01,0.001]\n",
    "\n",
    "# learning rate, epsilon\n",
    "epsilon_list = [0.0005, 0.00005, 0.000005]\n",
    "\n",
    "# training epochs\n",
    "epochs = 100\n",
    "\n",
    "#Use Hannahs encoding loading\n",
    "encoding_file = 'data/BLOSUM50' # could change it to data/sparse\n",
    "\n",
    "# Random seed \n",
    "np.random.seed( 1 )\n",
    "\n",
    "# early stopping\n",
    "early_stopping = True\n",
    "\n",
    "# partition files\n",
    "partition_files = ['data/partition_3.txt', 'data/partition_2.txt', 'data/partition_6.txt', 'data/partition_5.txt', 'data/partition_4.txt']\n",
    "\n",
    "#outer and inner cv partition folds\n",
    "K1, K2 = 5, 4\n",
    "    \n",
    "# define outer partitions\n",
    "outer_partitions = assign_cv_partition(partition_files, n_folds=K1)\n",
    "for k, (outer_train, outer_test) in enumerate(outer_partitions):\n",
    "    \n",
    "    #get outer training set from outer partition for training with optimal parameters\n",
    "    outer_peptides, outer_y = data_partition(outer_train, data=data, blosum_file=encoding_file)\n",
    "    \n",
    "    #get validation set from the outer partition to validate model one\n",
    "    validation_peptides, validation_targets = data_partition(outer_test, data=data, blosum_file=encoding_file)\n",
    "    \n",
    "    # make inner partition of the training set for parameter optimsiation\n",
    "    inner_partitions = assign_cv_partition(outer_train, n_folds=K2)\n",
    "    \n",
    "    \n",
    "    #Create storage for predictions for concatenation of predictions vs targets\n",
    "    storage_pred = make_storage(lamb_list,epsilon_list)\n",
    "    storage_target = make_storage(lamb_list,epsilon_list)\n",
    "    \n",
    "    for j, (inner_train, inner_test) in enumerate(inner_partitions):\n",
    "\n",
    "        # peptides for training\n",
    "        peptides, y = data_partition(outer_train, data=data, blosum_file=encoding_file)\n",
    "        N = len(peptides)\n",
    "        \n",
    "        \n",
    "        # evaluation peptide values\n",
    "        evaluation_peptides, evaluation_targets = data_partition(outer_test, data=data, blosum_file=encoding_file)\n",
    "        \n",
    "\n",
    "        #for each lambda and epsilon combi\n",
    "        for l in lamb_list:\n",
    "            for epsilon in epsilon_list:\n",
    "                lamb_N = l/N\n",
    "                stopping_error = np.inf # error for early stopping\n",
    "                # weights\n",
    "                input_dim  = len(peptides[0])\n",
    "                w_bound = 0.1\n",
    "                weights = np.random.uniform(-w_bound, w_bound, size=input_dim)                \n",
    "\n",
    "\n",
    "                # for each training epoch\n",
    "                for e in range(0, epochs):\n",
    "\n",
    "                    # for each peptide\n",
    "                    for i in range(0, N):\n",
    "\n",
    "                        # random index\n",
    "                        ix = np.random.randint(0, N)\n",
    "\n",
    "                        # get peptide       \n",
    "                        peptide = peptides[ix]\n",
    "\n",
    "                        # get target prediction value\n",
    "                        y_target = y[ix]\n",
    "                        #print(y_target)\n",
    "                        # get initial prediction\n",
    "                        y_pred = np.dot(peptide, weights)\n",
    "                        #print(y_pred)\n",
    "                        # gradient descent \n",
    "                        gradient_descent(y_pred, y_target, peptide, weights, lamb_N, epsilon) # updates weights\n",
    "\n",
    "                    #compute error, needed for plot\n",
    "                    #gerr, mse = cumulative_error(peptides, y, l, weights) \n",
    "\n",
    "                    # predict on training data, for plots\n",
    "                    #train_pred = predict( peptides, weights )\n",
    "                    #train_mse = cal_mse( y, train_pred )\n",
    "                    #train_pcc = pearsonr( y, train_pred )\n",
    "\n",
    "                    # predict on evaluation data, needed for early stopping\n",
    "                    eval_pred = predict(evaluation_peptides, weights )\n",
    "                    eval_mse = cal_mse(evaluation_targets, eval_pred )\n",
    "                    #eval_pcc = pearsonr(evaluation_targets, eval_pred) not needed here\n",
    "\n",
    "                    # early stopping\n",
    "                    if early_stopping:\n",
    "\n",
    "                        if eval_mse < stopping_error:\n",
    "\n",
    "                            stopping_error = eval_mse # save to compare future loops\n",
    "                            #stopping_pcc = eval_pcc[0] # save to compare with best pcc\n",
    "                            stopping_pred = eval_pred[:] # will this create an alias? add slice\n",
    "\n",
    "                            #print (\"# Save network\", e, \"Best MSE\", stopping_error, \"PCC\", stopping_pcc)\n",
    "                    \n",
    "                    #Wrong way finding best lambda\n",
    "                    #if stopping_pcc > best_pcc:\n",
    "                    #    best_pcc = stopping_pcc\n",
    "                    #    best_lamb = l\n",
    "                    #    best_epsilon = epsilon\n",
    "                    \n",
    "                    #print(\"Epoch: \", e, \"Gerr:\", gerr, train_pcc[0], train_mse, eval_pcc[0], eval_mse)\n",
    "                #print(\"Lambda \", l, \"Epsilon \",epsilon,\"PCC \",stopping_pcc, \"Outer \",k, \"Inner\",j)\n",
    "                \n",
    "                #save hyperparameter iteration to concatenated prediction vs target objects\n",
    "                storage_pred[l][epsilon].extend(stopping_pred)\n",
    "                storage_target[l][epsilon].extend(evaluation_targets)\n",
    "                \n",
    "                #print(storage_matrix)\n",
    "        \n",
    "    #Select best model from inner cv loop\n",
    "    best_pcc = -np.inf  #we are not training towards negative correlation\n",
    "    \n",
    "    #calculate pcc for concatenated predictions, save and print the best one\n",
    "    for ll in lamb_list:\n",
    "        for el in epsilon_list:\n",
    "            concat_pcc = pearsonr(storage_target[ll][el], storage_pred[ll][el])[0]\n",
    "            #print(ll,el,concat_pcc)\n",
    "            if concat_pcc > best_pcc:\n",
    "                best_pcc = concat_pcc\n",
    "                best_lamb = ll\n",
    "                best_epsilon = el\n",
    "    print(\"Best PCC \", best_pcc, \"Best Lamba \", best_lamb,\"Best epsilon \",best_epsilon)\n",
    "    \n",
    "    # train on outer test set\n",
    "    N = len(outer_peptides)\n",
    "    lamb=best_lamb\n",
    "    lamb_N = lamb/N\n",
    "    epsilon=best_epsilon\n",
    "    stopping_error = np.inf # for early stopping\n",
    "    # weights\n",
    "    input_dim  = len(outer_peptides[0])\n",
    "    w_bound = 0.1\n",
    "    weights = np.random.uniform(-w_bound, w_bound, size=input_dim)\n",
    "    best_weights = np.zeros(input_dim)\n",
    "                \n",
    "    # for each training epoch\n",
    "    for e in range(0, epochs):\n",
    "\n",
    "        # for each peptide\n",
    "        for i in range(0, N):\n",
    "\n",
    "            # random index\n",
    "            ix = np.random.randint(0, N)\n",
    "\n",
    "            # get peptide       \n",
    "            peptide = outer_peptides[ix]\n",
    "\n",
    "            # get target prediction value\n",
    "            y_target = outer_y[ix]\n",
    "            #print(y_target)\n",
    "            # get initial prediction\n",
    "            y_pred = np.dot(peptide, weights)\n",
    "            #print(y_pred)\n",
    "            # gradient descent \n",
    "            gradient_descent(y_pred, y_target, peptide, weights, lamb_N, epsilon) # updates weights\n",
    "\n",
    "        #compute error, needed for plot\n",
    "        #gerr, mse = cumulative_error(outer_peptides, outer_y, lamb, weights) \n",
    "\n",
    "        # predict on training data, only needed for plots\n",
    "        #train_pred = predict( outer_peptides, weights )\n",
    "        #train_mse = cal_mse( outer_y, train_pred )\n",
    "        #train_pcc = pearsonr( outer_y, train_pred )\n",
    "\n",
    "        # predict on outer test (validation data)\n",
    "        eval_pred = predict(validation_peptides, weights )\n",
    "        eval_mse = cal_mse(validation_targets, eval_pred )\n",
    "        #eval_pcc = pearsonr(validation_targets, eval_pred) don't calculate it unless needed\n",
    "\n",
    "        # early stopping\n",
    "        if early_stopping:\n",
    "\n",
    "            if eval_mse < stopping_error:\n",
    "\n",
    "                stopping_error = eval_mse # save to compare future loops\n",
    "                stopping_pcc = pearsonr(validation_targets, eval_pred)[0]\n",
    "                best_weights = weights[:]\n",
    "\n",
    "    print(\"Lambda: \", lamb,\"Epsilon: \", epsilon, \"PCC: \", stopping_pcc)\n",
    "    print(best_weights)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "smm_gradient_descent_v2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
